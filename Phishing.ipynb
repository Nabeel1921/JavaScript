{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNkLPzmkl21KcJeSZ5yxyt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nabeel1921/JavaScript/blob/main/Phishing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvJum8mFCQlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563390cb-5250-4bea-a689-ceca044bc37d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-10 03:01:49--  https://raw.githubusercontent.com/maximsachs/phishing_classification_recurrent_nn/master/combined_online_valid.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5219769 (5.0M) [text/plain]\n",
            "Saving to: ‘combined_online_valid.csv’\n",
            "\n",
            "combined_online_val 100%[===================>]   4.98M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-04-10 03:01:49 (57.6 MB/s) - ‘combined_online_valid.csv’ saved [5219769/5219769]\n",
            "\n",
            "--2023-04-10 03:01:49--  https://raw.githubusercontent.com/maximsachs/phishing_classification_recurrent_nn/master/top-1m_umbrella.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28869301 (28M) [text/plain]\n",
            "Saving to: ‘top-1m_umbrella.csv’\n",
            "\n",
            "top-1m_umbrella.csv 100%[===================>]  27.53M   150MB/s    in 0.2s    \n",
            "\n",
            "2023-04-10 03:01:50 (150 MB/s) - ‘top-1m_umbrella.csv’ saved [28869301/28869301]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Getting the data from: https://github.com/maximsachs/phishing_classification_recurrent_nn\n",
        "!wget https://raw.githubusercontent.com/maximsachs/phishing_classification_recurrent_nn/master/combined_online_valid.csv\n",
        "!wget https://raw.githubusercontent.com/maximsachs/phishing_classification_recurrent_nn/master/top-1m_umbrella.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from prettytable import PrettyTable\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "HhSJioyYCn2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some high level parameters:\n",
        "show_top_n = 20\n",
        "random_seed = 16\n",
        "\n",
        "# Setting the random seed so that the code is repeatable.\n",
        "np.random.seed(random_seed)"
      ],
      "metadata": {
        "id": "WIt1kbrbCtFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding\n",
        "\n",
        "# Some high-level parameters:\n",
        "show_top_n = 20\n",
        "random_seed = 16\n",
        "\n",
        "# Setting the random seed so that the code is repeatable.\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "vocab_size=20\n",
        "embedding_dim=100\n",
        "max_length=100\n",
        "\n",
        "# Add an Embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# Add a stacked LSTM layer with return_sequences=True\n",
        "model.add(LSTM(units=lstm_units, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units, return_sequences=True))  # Add additional LSTM layer for stacking\n",
        "\n",
        "# Add a GRU layer\n",
        "model.add(GRU(units=gru_units))\n",
        "\n",
        "# Add a Dense output layer\n",
        "model.add(Dense(units=output_units, activation=output_activation))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "cGTYzSvxIM1j",
        "outputId": "a7d5db8b-4b54-47b4-ad92-2085190b984a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-415a225b5141>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Add a stacked LSTM layer with return_sequences=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add additional LSTM layer for stacking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm_units' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the current combined online-valid dataset.\n",
        "# Use the dataset_downloader.py to download the current Phishtank online-valid.csv and update the combined dataset.\n",
        "print(\"Phishtank Online Valid Dataset\")\n",
        "online_valid_df = pd.read_csv(\"combined_online_valid.csv\")\n",
        "online_valid_df"
      ],
      "metadata": {
        "id": "7E1eJ2fYCu1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting tld and domain name.\n",
        "tld_count = defaultdict(lambda: 0)\n",
        "domain_names = []\n",
        "for index, row in online_valid_df.iterrows():\n",
        "    # Extracting the tld from the url\n",
        "    domain_name = row[\"url\"].replace(\"https://\",\"\").replace(\"http://\",\"\").split(\"/\")[0]\n",
        "    domain_names.append(domain_name)\n",
        "    tld = domain_name.split(\".\")[-1]\n",
        "    tld_count[tld] += 1\n",
        "\n",
        "tld_df = pd.Series(dict(tld_count))\n",
        "tld_df.sort_values(ascending=False, inplace=True)\n",
        "tld_print = tld_df.iloc[:show_top_n]\n",
        "tld_print[\"OTHERS\"] = tld_df.iloc[show_top_n:].sum()"
      ],
      "metadata": {
        "id": "GGmBYLA5Cz3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the domain names extracted from the phishing urls as a new column.\n",
        "online_valid_df[\"domain_names\"] = domain_names"
      ],
      "metadata": {
        "id": "EzeZ6azQEDlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the whitelist from the 1 million most frequently visited domains.\n",
        "whitelist_file_umbrella = \"top-1m_umbrella.csv\"\n",
        "whitelist_df = pd.read_csv(whitelist_file_umbrella, header=None, names=[\"rank\", \"domain_names\"])"
      ],
      "metadata": {
        "id": "LtR9i8KBEEs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding if there are any domains that are also in the whitelist.\n",
        "domains_in_whitelist = np.intersect1d(online_valid_df[\"domain_names\"], whitelist_df[\"domain_names\"])\n",
        "# Tagging the whitelisted domains as such.\n",
        "online_valid_df[\"in_whitelist\"] = np.in1d(online_valid_df[\"domain_names\"], domains_in_whitelist)"
      ],
      "metadata": {
        "id": "MAiTCaLuEMxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing some data examples for reference.\n",
        "print(online_valid_df.shape[0], \"rows\")\n",
        "print(tld_print.to_frame(name=\"TLD Count\").transpose().to_string())\n",
        "print(f\"Percentage of top {show_top_n} tlds: {np.round(100*tld_df.iloc[:show_top_n].sum()/tld_df.sum(), decimals=2)} %\")\n",
        "online_valid_df.head(20)"
      ],
      "metadata": {
        "id": "F2MWCuutEPvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the top of the whitelist.\n",
        "print(\"Whitelist file:\", whitelist_file_umbrella)\n",
        "print(whitelist_df.shape[0], \"rows\")\n",
        "print(whitelist_df.head(20))"
      ],
      "metadata": {
        "id": "6xb7pWZiETfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the dataset, excluding all where the domain name is in the whitelist.\n",
        "# Since if the domain is in both lists we cannot tell if its safe or phishing? --> This helps with labeleling the data.\n",
        "online_valid_df_without_intersection = online_valid_df.loc[online_valid_df['in_whitelist'] == False]\n",
        "whitelist_df_without_intersection = whitelist_df.loc[np.invert(whitelist_df['domain_names'].isin(domains_in_whitelist))]"
      ],
      "metadata": {
        "id": "CGuewmtFEaPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set oversampling_rate to 1 to have the positive samples match the phishing samples. Set to greater than 1 to use more negative samples.\n",
        "oversampling_rate = 1.5\n",
        "\n",
        "# Getting the array of all phishing domain names.\n",
        "phishing_domains = online_valid_df_without_intersection[\"domain_names\"].values\n",
        "# Randomly sample a number of safe urls, sice the ratio of classes in the training data should not be too much out of balance.\n",
        "whitelist_domains = np.random.choice(whitelist_df_without_intersection[\"domain_names\"].values, size=int(oversampling_rate*len(phishing_domains)), replace=False)"
      ],
      "metadata": {
        "id": "wMGCl83-Ec4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Selected Data Examples:\")\n",
        "print(\"Phishing domains:\", phishing_domains, len(phishing_domains))\n",
        "print(\"Benign domains:\", whitelist_domains, len(whitelist_domains))"
      ],
      "metadata": {
        "id": "hvSuHYlbEi1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling a phishing url 1 and a not-phishing url 0.\n",
        "# Using character encoding as the vocabulary.\n",
        "# Feeding the url as the sequence.\n",
        "# Creating the samples array and the label array\n",
        "print()\n",
        "X = list(phishing_domains) + list(whitelist_domains)\n",
        "y = [1]*len(phishing_domains) + [0]*len(whitelist_domains)\n",
        "sample_weights = [1]*len(phishing_domains) + [1/oversampling_rate]*len(whitelist_domains)"
      ],
      "metadata": {
        "id": "DilZbVGwEmaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding code/idea from TensorFlow 2.0 Complete Course - Python Neural Networks for Beginners Tutorial freeCodeCamp.org\n",
        "# https://colab.research.google.com/drive/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true\n",
        "vocab = sorted(set(\"\".join(X)), reverse=True)\n",
        "# Inserting a space at index 0, since it is not used in url and will be used for padding the examples.\n",
        "vocab.insert(0, \" \")\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print()\n",
        "print(f\"Encoding Vocabulary ({vocab_size}) used:\")\n",
        "print(vocab)\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "print(\"Encoding example:\")\n",
        "print(text_to_int(phishing_domains[0]))\n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_to_int(phishing_domains[0])))"
      ],
      "metadata": {
        "id": "rW66o0O3Eo4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigating the domain name length for the combined domain names:\n",
        "X_elem_len = [len(domain_name) for domain_name in X]\n",
        "print(sorted(X_elem_len, reverse=True)[:show_top_n])"
      ],
      "metadata": {
        "id": "l-Jr8ygWErr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting some max length for our urls.\n",
        "max_seq_len = 40\n",
        "print((np.array(X_elem_len) > max_seq_len).sum(), \"URLs longer than the cutoff length\", max_seq_len)"
      ],
      "metadata": {
        "id": "PfnlRExQEuDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, sample_weights_train, sample_weights_test = train_test_split(np.array(X), \n",
        "                                                                                               np.array(y),\n",
        "                                                                                               np.array(sample_weights),\n",
        "                                                                                               test_size=0.15,\n",
        "                                                                                               random_state=random_seed)"
      ],
      "metadata": {
        "id": "Ka7KWCQKEvvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing how many samples to print so printouts dont get so big.\n",
        "show_top_n = 5\n",
        "print(f\"Training and testing data: (showing first {show_top_n})\")\n",
        "print(f\"Train data {len(X_train)} samples\")\n",
        "print(list(zip(X_train[:show_top_n], y_train[:show_top_n], sample_weights_train[:show_top_n])))\n",
        "print(f\"Test data {len(X_test)} samples\")\n",
        "print(list(zip(X_test[:show_top_n], y_test[:show_top_n], sample_weights_test[:show_top_n])))"
      ],
      "metadata": {
        "id": "ypns0PivExiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the domain names using the vocabulary\n",
        "X_train_encoded = [text_to_int(domain_name) for domain_name in X_train]\n",
        "X_test_encoded = [text_to_int(domain_name) for domain_name in X_test]\n",
        "print()\n",
        "print(f\"Encoded data: (showing first {show_top_n})\")\n",
        "print(f\"Train data {len(X_train_encoded)} samples, encoded\")\n",
        "print(list(zip(X_train_encoded[:show_top_n], y_train[:show_top_n])))\n",
        "print(f\"Test data {len(X_test_encoded)} samples, encoded\")\n",
        "print(list(zip(X_test_encoded[:show_top_n], y_test[:show_top_n])))\n",
        "\n",
        "# Padding to the right sequence length.\n",
        "X_train_encoded_padded = sequence.pad_sequences(X_train_encoded, max_seq_len)\n",
        "X_test_encoded_padded = sequence.pad_sequences(X_test_encoded, max_seq_len)\n",
        "print()\n",
        "print(f\"Encoded and padded data: (showing first {show_top_n})\")\n",
        "print(f\"Train data {len(X_train_encoded_padded)} samples, encoded\")\n",
        "print(list(zip(X_train_encoded_padded[:show_top_n], y_train[:show_top_n])))\n",
        "print(f\"Test data {len(X_test_encoded_padded)} samples, encoded\")\n",
        "print(list(zip(X_test_encoded_padded[:show_top_n], y_test[:show_top_n])))"
      ],
      "metadata": {
        "id": "X6_TGCH-E2uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_nn_model(X, y, threshold=0.5, bins=5, graph_bins=15, examples_per_bin=15):\n",
        "    \"\"\"\n",
        "    Custom nn evaluation to get the TP, TN, FP, FN rates.\n",
        "    Anything below threshold is considered not phishing.\n",
        "    Anything above threshold is considered phishing.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(X).flatten()\n",
        "    mean_prediction = np.mean(predictions)\n",
        "    print(f\"Calculated {len(predictions)} predictions with a mean value of {mean_prediction}\")\n",
        "    print(f\"Evaluating using threshold {threshold}\")\n",
        "    # Turning the predictions into 0 and 1 by checking the threshold. (0 safe, 1 phishing)\n",
        "    predictions_boolean = predictions > threshold\n",
        "    predictions_binary = predictions_boolean.astype(np.int)\n",
        "    print(f\"Cut-off threshold: {np.round(threshold, decimals=4)}\")\n",
        "    groundtruth_elements, groundtruth_counts = np.unique(y, return_counts=True)\n",
        "    groundtruth_counts = dict(zip(groundtruth_elements, groundtruth_counts))\n",
        "    evaluation_ratios_counts, sample_outcomes = statistics_evaluator(predictions_binary, y)\n",
        "    statistics_table_printer(evaluation_ratios_counts)\n",
        "    # showing some examples for each type of outcome: 0 TN, 1 FP, 2 FN, 3 TP\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
        "    outcome_index = [0, 1, 2, 3]\n",
        "    outcome_plot_positions = [0, 1, 2, 3]\n",
        "    outcome_labels = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
        "    y_axis_max = 0\n",
        "    for outcome in outcome_index:\n",
        "        outcome_indexes = np.where(np.array(sample_outcomes) == outcome)[0]\n",
        "        # Instead of random samples, do a histogram with bins of the predictions for this outcome.\n",
        "        # Then sample examples from each bin.\n",
        "        outcome_predictions = predictions[outcome_indexes]\n",
        "        outcome_binary = [ int(ind) for ind in list(str(bin(outcome_plot_positions[outcome])).replace(\"0b\",\"\").rjust(2, \"0\"))]\n",
        "        outcome_hist, outcome_bins = np.histogram(outcome_predictions, bins=bins)\n",
        "        plot_hist, plot_bins = np.histogram(outcome_predictions, bins=graph_bins)\n",
        "        outcome_total_count = groundtruth_counts[outcome_binary[0]]\n",
        "        plot_hist = (100*np.array(plot_hist))/outcome_total_count\n",
        "        axs[outcome_binary[0], outcome_binary[1]].bar(plot_bins[:-1], plot_hist, width = plot_bins[1]-plot_bins[0], align=\"edge\")\n",
        "        y_axis_max = max( max(plot_hist), y_axis_max)\n",
        "        axs[outcome_binary[0], outcome_binary[1]].set_title(outcome_labels[outcome])\n",
        "        # Randomly sample some examples from each bin for this outcome:\n",
        "        for bin_start, bin_end in zip(outcome_bins[:-1], outcome_bins[1:]):\n",
        "            bin_outcome_indexes = np.where( np.logical_and( np.array(outcome_predictions) >= bin_start, np.array(outcome_predictions) < bin_end ))[0]\n",
        "            bin_outcome_indexes = outcome_indexes[bin_outcome_indexes]\n",
        "            if len(bin_outcome_indexes) > examples_per_bin:\n",
        "                chosen_bin_outcome_examples = np.random.choice(bin_outcome_indexes, size=examples_per_bin, replace=False)\n",
        "            else:\n",
        "                chosen_bin_outcome_examples = bin_outcome_indexes\n",
        "            example_truth = y[chosen_bin_outcome_examples]\n",
        "            example_input_encoded = X[chosen_bin_outcome_examples]\n",
        "            example_input_decoded = [ int_to_text(example).strip() for example in example_input_encoded]\n",
        "            example_prediction = predictions[chosen_bin_outcome_examples]\n",
        "            example_df = pd.DataFrame(data={\"input\": example_input_decoded, \"ground truth\": example_truth, \"prediction\": example_prediction})\n",
        "            print(\"\\nExamples for\", outcome_labels[outcome], \"Bin range:\", bin_start, \"-\", bin_end, \", Num. Samples:\", len(bin_outcome_indexes))\n",
        "            print(example_df.to_string())\n",
        "    for ax in axs.flat:\n",
        "        ax.set(xlabel='Prediction', ylabel='Percentage of samples')\n",
        "        ax.set_ylim(0, y_axis_max*1.02)\n",
        "        ax.grid()\n",
        "    plt.tight_layout()\n",
        "    fig.savefig('outcome_distributions.pdf')\n",
        "    return mean_prediction\n",
        "\n",
        "def statistics_evaluator(predictions_binary, y_binary):\n",
        "    # Concattenating the strings of the binary value of the prediction and the truth.\n",
        "    # First value is the prediction, second the actual label\n",
        "    # Hypothesis is: is phishing -> positive: yes phishing, negative: no phishing\n",
        "    # Then 00 would be a TN, 01 is a FP, 10 is a FN, 11 is a TP. \n",
        "    # Converting the binary outcomes to integer: 0 TN, 1 FP, 2 FN, 3 TP\n",
        "    hypothesis_tests = [int(str(label)+str(prediction), 2) for prediction, label in zip(predictions_binary, y_binary)]\n",
        "    # Counting the number of times each unique value in the tests is returned.\n",
        "    unique_elements, counts_elements = np.unique(hypothesis_tests, return_counts=True)\n",
        "    counts_elements = dict(zip(unique_elements, counts_elements))\n",
        "    outcome_labels = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
        "    evaluation_ratios_counts = dict(zip(outcome_labels, [counts_elements.get(0, 0), counts_elements.get(1, 0), counts_elements.get(2, 0), counts_elements.get(3, 0)]))\n",
        "    return evaluation_ratios_counts, hypothesis_tests\n",
        "\n",
        "def statistics_table_printer(evaluation_ratios_counts, decimals=3):\n",
        "    print(\"Evaluation counts:\", evaluation_ratios_counts)\n",
        "    try:\n",
        "        positive_predictive_value = evaluation_ratios_counts[\"TP\"]/(evaluation_ratios_counts[\"TP\"]+evaluation_ratios_counts[\"FP\"])\n",
        "    except:\n",
        "        positive_predictive_value = 0\n",
        "    try:\n",
        "        true_positive_rate = evaluation_ratios_counts[\"TP\"]/(evaluation_ratios_counts[\"TP\"]+evaluation_ratios_counts[\"FN\"])\n",
        "    except:\n",
        "        true_positive_rate = 0\n",
        "    try:\n",
        "        false_discovery_rate = evaluation_ratios_counts[\"FP\"]/(evaluation_ratios_counts[\"TP\"]+evaluation_ratios_counts[\"FP\"])\n",
        "    except:\n",
        "        false_discovery_rate = 0\n",
        "    try:\n",
        "        false_positive_rate = evaluation_ratios_counts[\"FP\"]/(evaluation_ratios_counts[\"FP\"]+evaluation_ratios_counts[\"TN\"])\n",
        "    except:\n",
        "        false_positive_rate = 0\n",
        "    try:\n",
        "        false_omission_rate = evaluation_ratios_counts[\"FN\"]/(evaluation_ratios_counts[\"TN\"]+evaluation_ratios_counts[\"FN\"])\n",
        "    except:\n",
        "        false_omission_rate = 0\n",
        "    try:\n",
        "        false_negative_rate = evaluation_ratios_counts[\"FN\"]/(evaluation_ratios_counts[\"TP\"]+evaluation_ratios_counts[\"FN\"])\n",
        "    except:\n",
        "        false_negative_rate = 0\n",
        "    try:\n",
        "        negative_predictive_value = evaluation_ratios_counts[\"TN\"]/(evaluation_ratios_counts[\"TN\"]+evaluation_ratios_counts[\"FN\"])\n",
        "    except:\n",
        "        negative_predictive_value = 0\n",
        "    try:\n",
        "        true_negative_rate = evaluation_ratios_counts[\"TN\"]/(evaluation_ratios_counts[\"TN\"]+evaluation_ratios_counts[\"FP\"])\n",
        "    except:\n",
        "        true_negative_rate = 0\n",
        "    try:\n",
        "        accuracy = (evaluation_ratios_counts.get(\"TP\",0)+evaluation_ratios_counts.get(\"TN\",0))/(evaluation_ratios_counts.get(\"TP\",0)+evaluation_ratios_counts.get(\"TN\",0) + evaluation_ratios_counts.get(\"FP\",0) + evaluation_ratios_counts.get(\"FN\",0))\n",
        "    except:\n",
        "        accuracy = 0\n",
        "    t = PrettyTable([f\"Accuracy {np.round(accuracy*100, decimals=decimals)}%\",\n",
        "                     'Predicted safe',\n",
        "                     'Predicted phishing'])\n",
        "    t.add_row([\"Not phishing\",\n",
        "               \"TN: {TN}\".format(**evaluation_ratios_counts),\n",
        "               \"FP: {FP}\".format(**evaluation_ratios_counts)])\n",
        "    t.add_row(['', f\"NPV: {np.round(negative_predictive_value*100, decimals=decimals)}%\",\n",
        "                   f\"FDR: {np.round(false_discovery_rate*100, decimals=decimals)}%\"])\n",
        "    t.add_row(['', f\"TNR: {np.round(true_negative_rate*100, decimals=decimals)}%\",\n",
        "                   f\"FPR: {np.round(false_positive_rate*100, decimals=decimals)}%\"])\n",
        "    t.add_row([\"Is phishing\",\n",
        "               \"FN: {FN}\".format(**evaluation_ratios_counts), \n",
        "               \"TP: {TP}\".format(**evaluation_ratios_counts)])\n",
        "    t.add_row(['', f\"FOR: {np.round(false_omission_rate*100, decimals=decimals)}%\",\n",
        "                   f\"PPV: {np.round(positive_predictive_value*100, decimals=decimals)}%\"])\n",
        "    t.add_row(['', f\"FNR: {np.round(false_negative_rate*100, decimals=decimals)}%\", \n",
        "                   f\"TPR: {np.round(true_positive_rate*100, decimals=decimals)}%\"])\n",
        "    print(t)\n",
        "\n",
        "def threshold_evaluation_plotter(X, y, min_threshold=0.05, max_threshold=0.95, steps=200, decimals=3):\n",
        "    predictions = model.predict(X).flatten()\n",
        "    stat_counts = []\n",
        "    # Sweeping over the ranges.\n",
        "    for threshold in np.linspace(min_threshold, max_threshold, steps):\n",
        "        predictions_boolean = predictions > threshold\n",
        "        predictions_binary = predictions_boolean.astype(np.int)\n",
        "        evaluation_ratios_counts, sample_outcomes = statistics_evaluator(predictions_binary, y)\n",
        "        stat_counts.append(evaluation_ratios_counts)\n",
        "    counts_df = pd.DataFrame(data=stat_counts, index=np.linspace(min_threshold, max_threshold, steps))\n",
        "    stat_df = pd.DataFrame(index=np.linspace(min_threshold, max_threshold, steps))\n",
        "    # Calculating the stats:\n",
        "    stat_df[\"accuracy\"] = (counts_df[\"TP\"]+counts_df[\"TN\"])/(counts_df[\"TP\"]+counts_df[\"TN\"]+counts_df[\"FP\"]+counts_df[\"FN\"])\n",
        "    stat_df[\"PPV\"] = counts_df[\"TP\"]/(counts_df[\"TP\"]+counts_df[\"FP\"])\n",
        "    stat_df[\"TPR\"] = counts_df[\"TP\"]/(counts_df[\"TP\"]+counts_df[\"FN\"])\n",
        "    stat_df[\"FDR\"] = counts_df[\"FP\"]/(counts_df[\"TP\"]+counts_df[\"FP\"])\n",
        "    stat_df[\"FPR\"] = counts_df[\"FP\"]/(counts_df[\"FP\"]+counts_df[\"TN\"])\n",
        "    stat_df[\"FOR\"] = counts_df[\"FN\"]/(counts_df[\"TN\"]+counts_df[\"FN\"])\n",
        "    stat_df[\"FNR\"] = counts_df[\"FN\"]/(counts_df[\"TP\"]+counts_df[\"FN\"])\n",
        "    stat_df[\"NPV\"] = counts_df[\"TN\"]/(counts_df[\"TN\"]+counts_df[\"FN\"])\n",
        "    stat_df[\"TNR\"] = counts_df[\"TN\"]/(counts_df[\"FP\"]+counts_df[\"TN\"])\n",
        "    fig = stat_df.plot(kind='line',  figsize=(20, 7), fontsize=16, lw=3).get_figure()\n",
        "    plt.tight_layout()\n",
        "    plt.grid()\n",
        "    fig.savefig('threshold_statistics_sweep.pdf')\n",
        "    print(\"Best performance at threshold:\", stat_df['accuracy'].idxmax())\n",
        "    return stat_df['accuracy'].idxmax()\n",
        "\n",
        "def predict_url(url):\n",
        "    encoded_text = sequence.pad_sequences([text_to_int(url)], max_seq_len)\n",
        "    result = model.predict(encoded_text) \n",
        "    print(\"Prediction on url:\", url, result[0][0])"
      ],
      "metadata": {
        "id": "tgrs2t6rFF6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 64),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(128,activation=\"tanh\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "m91FXgb6FJdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "vSivqbgAFRqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight={0: (1/(oversampling_rate+1)), 1: (oversampling_rate/(oversampling_rate+1))}\n",
        "print(\"Using the class weighting:\", class_weight)\n",
        "# Training the model\n",
        "# Setting up callback to monitor the selected loss, and stops training if it doesnt improve for patience-number of epochs.\n",
        "# After stopping training will restore the weights from the best iteration on this value encountered so far.\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "xb22npY5FUq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_encoded_padded, y_train,\n",
        "                    epochs=100,\n",
        "                    validation_data=(X_test_encoded_padded, y_test),\n",
        "                    class_weight=class_weight,\n",
        "                    sample_weight=sample_weights_train,\n",
        "                    callbacks=[early_stopping_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ0mvvKUFVoG",
        "outputId": "33919062-4b49-4128-db51-84720f0dc4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1550/1550 [==============================] - 132s 81ms/step - loss: 0.1909 - acc: 0.7189 - val_loss: 0.4641 - val_acc: 0.7739\n",
            "Epoch 2/100\n",
            "1550/1550 [==============================] - 122s 79ms/step - loss: 0.1642 - acc: 0.7674 - val_loss: 0.4384 - val_acc: 0.7813\n",
            "Epoch 3/100\n",
            "1550/1550 [==============================] - 130s 84ms/step - loss: 0.1552 - acc: 0.7822 - val_loss: 0.4733 - val_acc: 0.7576\n",
            "Epoch 4/100\n",
            " 723/1550 [============>.................] - ETA: 1:02 - loss: 0.1472 - acc: 0.7971"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model built in evaluate\n",
        "results = model.evaluate(X_test_encoded_padded, y_test)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "pb0UsttNFZtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing some handcrafted examples to see how it does.\n",
        "print(\"\\nPhishing ULR examples:\")\n",
        "predict_url(\"frgcxtmjawefgrthdcusge.dab\")\n",
        "predict_url(\"evilmadeupurl.phish\")\n",
        "predict_url(\"evil.madeupurl.phish\")\n",
        "\n",
        "print(\"\\nSafe URL examples:\")\n",
        "predict_url(\"google.com\")\n",
        "predict_url(\"www.google.com\")\n",
        "predict_url(\"gmail.google.com\")\n",
        "predict_url(\"mail.google.com\")\n",
        "predict_url(\"tudelft.nl\")\n",
        "predict_url(\"brightspace.tudelft.nl\")\n",
        "predict_url(\"colab.research.google.com\")\n",
        "predict_url(\"00-gayrettepe-t3-8---00-gayrettepe-xrs-t2-1.statik.turktelekom.com.tr\")"
      ],
      "metadata": {
        "id": "UfANAuhaFaXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom evaluate\n",
        "best_threshold = threshold_evaluation_plotter(X_test_encoded_padded, y_test)\n",
        "mean_prediction = evaluate_nn_model(X_test_encoded_padded, y_test, threshold=best_threshold)"
      ],
      "metadata": {
        "id": "J_DV45d0FaX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0BiTHxuFe3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}